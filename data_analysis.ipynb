{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import time\n",
    "from statistics import mode\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "import os\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "print_cnt = 0\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def process_text(text):\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 数詞を数字に変換\n",
    "    num_word_to_digit = {\n",
    "        'zero': '0', 'one': '1', 'two': '2', 'three': '3', 'four': '4',\n",
    "        'five': '5', 'six': '6', 'seven': '7', 'eight': '8', 'nine': '9',\n",
    "        'ten': '10'\n",
    "    }\n",
    "    for word, digit in num_word_to_digit.items():\n",
    "        text = text.replace(word, digit)\n",
    "\n",
    "    # 小数点のピリオドを削除\n",
    "    text = re.sub(r'(?<!\\d)\\.(?!\\d)', '', text)\n",
    "\n",
    "    # 冠詞の削除\n",
    "    text = re.sub(r'\\b(a|an|the)\\b', '', text)\n",
    "\n",
    "    # 短縮形のカンマの追加\n",
    "    contractions = {\n",
    "        \"dont\": \"don't\", \"isnt\": \"isn't\", \"arent\": \"aren't\", \"wont\": \"won't\",\n",
    "        \"cant\": \"can't\", \"wouldnt\": \"wouldn't\", \"couldnt\": \"couldn't\"\n",
    "    }\n",
    "    for contraction, correct in contractions.items():\n",
    "        text = text.replace(contraction, correct)\n",
    "\n",
    "    # 句読点をスペースに変換\n",
    "    text = re.sub(r\"[^\\w\\s':]\", ' ', text)\n",
    "\n",
    "    # 句読点をスペースに変換\n",
    "    text = re.sub(r'\\s+,', ',', text)\n",
    "\n",
    "    # 連続するスペースを1つに変換\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def is_color_question(text):\n",
    "    words = ['what color', 'what _colors']\n",
    "    return int(any((w in text) for w in words))\n",
    "\n",
    "def is_closed_question(text):\n",
    "    words1 = ['are you', 'can you', 'will you', 'could you', 'aren\\'t you']\n",
    "    return int(text[:1] == 'is' or text[:3] == 'does' or (text.find('you') >= 0) and any((w in text) for w in words1))\n",
    "\n",
    "\n",
    "# CSVファイルからクラスマッピングを読み込む\n",
    "def load_class_mapping():\n",
    "    df = pd.read_csv('class_mapping.csv')\n",
    "    class_mapping = {row['answer']: int(row['class_id']) for index, row in df.iterrows()}\n",
    "    return class_mapping\n",
    "\n",
    "# 1. データローダーの作成\n",
    "class VQADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df_path, image_dir, transform=None, answer=True):\n",
    "        self.transform = transform  # 画像の前処理\n",
    "        self.image_dir = image_dir  # 画像ファイルのディレクトリ\n",
    "        self.df = pd.read_json(df_path)  # 画像ファイルのパス，question, answerを持つDataFrame\n",
    "        self.df['question'] = self.df['question'].apply(process_text)\n",
    "        self.df['color'] = self.df['question'].apply(is_color_question)\n",
    "        self.df['closed'] = self.df['question'].apply(is_closed_question)\n",
    "        self.answer = answer\n",
    "\n",
    "        # answerの辞書を作成\n",
    "        self.answer2idx = load_class_mapping()\n",
    "        self.idx2answer = {}\n",
    "\n",
    "        if self.answer:\n",
    "            # 回答に含まれる単語を辞書に追加\n",
    "            for answers in self.df[\"answers\"]:\n",
    "                for answer in answers:\n",
    "                    word = answer[\"answer\"]\n",
    "                    word = process_text(word)\n",
    "                    if word not in self.answer2idx:\n",
    "                        self.answer2idx[word] = len(self.answer2idx)\n",
    "            self.idx2answer = {v: k for k, v in self.answer2idx.items()}  # 逆変換用の辞書(answer)\n",
    "\n",
    "    def update_dict(self, dataset):\n",
    "        \"\"\"\n",
    "        検証用データ，テストデータの辞書を訓練データの辞書に更新する．\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : Dataset\n",
    "            訓練データのDataset\n",
    "        \"\"\"\n",
    "        # self.question2idx = dataset.question2idx\n",
    "        self.answer2idx = dataset.answer2idx\n",
    "        # self.idx2question = dataset.idx2question\n",
    "        self.idx2answer = dataset.idx2answer\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        対応するidxのデータ（画像，質問，回答）を取得．\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : int\n",
    "            取得するデータのインデックス\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        image : torch.Tensor  (C, H, W)\n",
    "            画像データ\n",
    "        question : torch.Tensor  (vocab_size)\n",
    "            質問文をone-hot表現に変換したもの\n",
    "        answers : torch.Tensor  (n_answer)\n",
    "            10人の回答者の回答のid\n",
    "        mode_answer_idx : torch.Tensor  (1)\n",
    "            10人の回答者の回答の中で最頻値の回答のid\n",
    "        \"\"\"\n",
    "        image = Image.open(f\"{self.image_dir}/{self.df['image'][idx]}\")\n",
    "        image = self.transform(image)\n",
    "        \n",
    "        question = self.df['question'][idx]\n",
    "        color = self.df['color'][idx]\n",
    "        closed = self.df['closed'][idx]\n",
    "\n",
    "\n",
    "        if self.answer:\n",
    "            answers = [self.answer2idx[process_text(answer['answer'])] for answer in self.df['answers'][idx]]\n",
    "            mode_answer_idx = mode(answers)  # 最頻値を取得（正解ラベル）\n",
    "\n",
    "            return image, question, color, closed, torch.Tensor(answers).to(torch.int32), int(mode_answer_idx)\n",
    "            # return image, question, torch.Tensor(answers), int(mode_answer_idx)\n",
    "\n",
    "        else:\n",
    "            return image, question, color, closed\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    \n",
    "    \n",
    "set_seed(42)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# dataloader / model\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomRotation(degrees=(-180, 180)),  # random rotation\n",
    "    transforms.RandomCrop(32, padding=(4, 4, 4, 4), padding_mode='constant'),  # random cropping\n",
    "    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5),\n",
    "    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "    \n",
    "train_dataset = VQADataset(df_path=\"./data/train.json\", image_dir=\"./data/train\", transform=transform)\n",
    "test_dataset = VQADataset(df_path=\"./data/valid.json\", image_dir=\"./data/valid\", transform=transform, answer=False)\n",
    "test_dataset.update_dict(train_dataset)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=os.cpu_count(), pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=os.cpu_count(), pin_memory=True)\n",
    "model = VQAModel(n_answer=len(train_dataset.answer2idx)).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
      "        1, 0, 0, 1, 0, 0, 0, 0]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "for image, question, color, closed, answers, mode_answer in train_loader:\n",
    "    image, answers, mode_answer = \\\n",
    "        image.to(device, non_blocking=True), answers.to(device, non_blocking=True), mode_answer.to(device, non_blocking=True)\n",
    "#     pred = model(image, question, color, closed)\n",
    "    print(color, closed)\n",
    "#     loss = criterion(pred, mode_answer.squeeze())\n",
    "\n",
    "#     answer_counts = torch.zeros(pred.shape, device=image.device)\n",
    "#     for i in range(pred.shape[0]):\n",
    "#         for answer in answers[i]:\n",
    "#             answer_counts[i][answer] += 1\n",
    "\n",
    "#     print(mode_answer.squeeze())\n",
    "#     print(pred.shape)\n",
    "#     print(mode_answer.shape, mode_answer.squeeze().shape)\n",
    "#     print(answer_counts.shape, answer_counts.squeeze().shape)\n",
    "    \n",
    "#     print(F.cross_entropy(pred, mode_answer.squeeze()))\n",
    "#     loss = F.cross_entropy(pred, answer_counts.squeeze())\n",
    "#     print(loss)\n",
    "\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     total_loss += loss.item()\n",
    "#     total_acc += VQA_criterion(pred.argmax(1), answers)  # VQA accuracy\n",
    "#     simple_acc += (pred.argmax(1) == mode_answer).float().mean().item()  # simple accuracy\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [{'answer_confidence': 'yes', 'answer': 'beef ...\n",
       "1        [{'answer_confidence': 'yes', 'answer': 'unans...\n",
       "2        [{'answer_confidence': 'yes', 'answer': 'grey'...\n",
       "3        [{'answer_confidence': 'maybe', 'answer': 'lot...\n",
       "4        [{'answer_confidence': 'yes', 'answer': 'no'},...\n",
       "                               ...                        \n",
       "19868    [{'answer_confidence': 'yes', 'answer': 'unans...\n",
       "19869    [{'answer_confidence': 'yes', 'answer': 'finge...\n",
       "19870    [{'answer_confidence': 'yes', 'answer': 'hands...\n",
       "19871    [{'answer_confidence': 'yes', 'answer': 'unans...\n",
       "19872    [{'answer_confidence': 'maybe', 'answer': 'yes...\n",
       "Name: answers, Length: 19873, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.df[\"answers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             question  \\\n",
      "1   maybe it's because you're pushing it down instead   \n",
      "3   can you tell me if this is like body wash or l...   \n",
      "4                                         is it paper   \n",
      "7                                 what device is this   \n",
      "8                         please tell me what this is   \n",
      "11                                       what is this   \n",
      "14                             what's in this package   \n",
      "16                                 what's in this can   \n",
      "17                                       what is this   \n",
      "19                                       what is this   \n",
      "20                              when does this expire   \n",
      "23                                       what is this   \n",
      "24                                  what book is this   \n",
      "25                           what kind of box is this   \n",
      "27                can you please tell me what this is   \n",
      "28                          my routerthe front lights   \n",
      "29                         you read directions on box   \n",
      "31                what color what color is this shirt   \n",
      "32           how long do i cook this for in microwave   \n",
      "33               for what college team is this helmet   \n",
      "\n",
      "                                              answers  \n",
      "1   [{'answer_confidence': 'yes', 'answer': 'unans...  \n",
      "3   [{'answer_confidence': 'maybe', 'answer': 'lot...  \n",
      "4   [{'answer_confidence': 'yes', 'answer': 'no'},...  \n",
      "7   [{'answer_confidence': 'maybe', 'answer': 'cam...  \n",
      "8   [{'answer_confidence': 'yes', 'answer': 'compu...  \n",
      "11  [{'answer_confidence': 'yes', 'answer': 'can g...  \n",
      "14  [{'answer_confidence': 'yes', 'answer': 'unans...  \n",
      "16  [{'answer_confidence': 'yes', 'answer': 'gt'},...  \n",
      "17  [{'answer_confidence': 'yes', 'answer': 'tv'},...  \n",
      "19  [{'answer': 'browser window on computer', 'ans...  \n",
      "20  [{'answer': 'unanswerable', 'answer_confidence...  \n",
      "23  [{'answer': 'unanswerable', 'answer_confidence...  \n",
      "24  [{'answer_confidence': 'yes', 'answer': 'unans...  \n",
      "25  [{'answer_confidence': 'yes', 'answer': 'milk'...  \n",
      "27  [{'answer_confidence': 'yes', 'answer': 'brush...  \n",
      "28  [{'answer_confidence': 'no', 'answer': 'unansw...  \n",
      "29  [{'answer_confidence': 'yes', 'answer': 'unans...  \n",
      "31  [{'answer_confidence': 'yes', 'answer': 'unans...  \n",
      "32  [{'answer_confidence': 'yes', 'answer': 'unans...  \n",
      "33  [{'answer': 'unanswerable', 'answer_confidence...  \n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.df[train_dataset.df[\"answers\"].apply(lambda ls: \"unanswerable\" in list(map(lambda ls: ls[\"answer\"], ls)))][[\"question\", \"answers\"]][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              question  \\\n",
      "433          could you tell what brand of wine this is   \n",
      "438                         are these mushrooms edible   \n",
      "445                  can you read label of this bottle   \n",
      "459  can you please tell me who this business card ...   \n",
      "495                 can you tell me what's in this can   \n",
      "507          can you tell what kind of pills these are   \n",
      "509                   can you tell me who this is from   \n",
      "520                    can you see what is in this can   \n",
      "521                           end of sandstorm is this   \n",
      "535   and what are your hours or do you work all night   \n",
      "537               can i use this to wash kitchen floor   \n",
      "538                   is there picture on this blanket   \n",
      "542  is text on this page inverted and what page nu...   \n",
      "545                   can you read what is on this bag   \n",
      "546                          how do i do are they even   \n",
      "564                   can you tell me what this can is   \n",
      "567  can you tell me sodium con10t of this can of food   \n",
      "572                             is computer monitor on   \n",
      "577        can you read make and model of battery case   \n",
      "588  hi can you see cooking instructions for this dish   \n",
      "\n",
      "                                               answers  \n",
      "433  [{'answer_confidence': 'yes', 'answer': 'balla...  \n",
      "438  [{'answer': 'unanswerable', 'answer_confidence...  \n",
      "445  [{'answer_confidence': 'yes', 'answer': 'luxar...  \n",
      "459  [{'answer': 'unreadable', 'answer_confidence':...  \n",
      "495  [{'answer_confidence': 'no', 'answer': 'unansw...  \n",
      "507  [{'answer_confidence': 'maybe', 'answer': 'no'...  \n",
      "509  [{'answer_confidence': 'no', 'answer': 'unansw...  \n",
      "520  [{'answer_confidence': 'yes', 'answer': 'no'},...  \n",
      "521  [{'answer_confidence': 'maybe', 'answer': 'una...  \n",
      "535  [{'answer_confidence': 'yes', 'answer': 'hours...  \n",
      "537  [{'answer_confidence': 'maybe', 'answer': 'if ...  \n",
      "538  [{'answer_confidence': 'yes', 'answer': 'yes'}...  \n",
      "542  [{'answer_confidence': 'yes', 'answer': 'can n...  \n",
      "545  [{'answer_confidence': 'yes', 'answer': 'no'},...  \n",
      "546  [{'answer_confidence': 'maybe', 'answer': 'una...  \n",
      "564  [{'answer_confidence': 'yes', 'answer': 'can c...  \n",
      "567  [{'answer': 'unanswerable', 'answer_confidence...  \n",
      "572  [{'answer_confidence': 'yes', 'answer': 'unans...  \n",
      "577  [{'answer_confidence': 'yes', 'answer': 'no'},...  \n",
      "588  [{'answer_confidence': 'yes', 'answer': 'no to...  \n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.df[train_dataset.df[\"answers\"].apply(lambda ls: any(w in list(map(lambda ls: ls[\"answer\"], ls)) for w in ['yes', 'no'] ) )][[\"question\", \"answers\"]][40:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0           beef chuck steak\n",
      "1               unanswerable\n",
      "2                       grey\n",
      "3                     lotion\n",
      "4                         no\n",
      "                ...         \n",
      "19868           unanswerable\n",
      "19869          finger puppet\n",
      "19870    handsome like daddy\n",
      "19871           unanswerable\n",
      "19872                     no\n",
      "Name: answers, Length: 19873, dtype: object\n",
      "unanswerable       7279\n",
      "no                  521\n",
      "yes                 448\n",
      "white               291\n",
      "grey                271\n",
      "black               237\n",
      "blue                218\n",
      "red                 116\n",
      "brown               106\n",
      "pink                 90\n",
      "keyboard             90\n",
      "green                78\n",
      "laptop               70\n",
      "purple               64\n",
      "dog                  63\n",
      "phone                51\n",
      "soup                 50\n",
      "cell phone           50\n",
      "yellow               47\n",
      "coca cola            44\n",
      "remote               40\n",
      "coffee               39\n",
      "lotion               39\n",
      "nothing              38\n",
      "wine                 38\n",
      "corn                 38\n",
      "computer screen      37\n",
      "orange               37\n",
      "computer             36\n",
      "pepsi                33\n",
      "Name: answers, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "answers_series = train_dataset.df[\"answers\"].apply(lambda ls: np.array([e[\"answer\"] for e in ls]))\n",
    "\n",
    "# numpy 配列から最頻値を求める関数\n",
    "def get_mode(array):\n",
    "    vals, counts = np.unique(array, return_counts=True)\n",
    "    max_index = counts.argmax()\n",
    "    return vals[max_index]\n",
    "\n",
    "# 最頻値を格納する新しい Series を作成\n",
    "mode_series = answers_series.apply(get_mode)\n",
    "\n",
    "# 最頻値の出現数をカウント\n",
    "frequency = mode_series.value_counts()\n",
    "\n",
    "# 結果の表示\n",
    "print(mode_series)\n",
    "print(frequency[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7279       1\n",
       "521        1\n",
       "78         1\n",
       "448        1\n",
       "291        1\n",
       "271        1\n",
       "237        1\n",
       "116        1\n",
       "106        1\n",
       "218        1\n",
       "70         1\n",
       "64         1\n",
       "31         1\n",
       "51         1\n",
       "47         1\n",
       "44         1\n",
       "40         1\n",
       "36         1\n",
       "63         1\n",
       "33         1\n",
       "29         2\n",
       "90         2\n",
       "50         2\n",
       "39         2\n",
       "37         2\n",
       "32         2\n",
       "26         2\n",
       "22         2\n",
       "21         2\n",
       "17         2\n",
       "24         2\n",
       "23         3\n",
       "38         3\n",
       "27         3\n",
       "20         4\n",
       "19         4\n",
       "11         6\n",
       "15         7\n",
       "12         7\n",
       "16         9\n",
       "18         9\n",
       "13        10\n",
       "14        11\n",
       "9         20\n",
       "10        22\n",
       "8         25\n",
       "7         33\n",
       "6         43\n",
       "5         54\n",
       "4        117\n",
       "3        230\n",
       "2        571\n",
       "1       4131\n",
       "Name: answers, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency.value_counts().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(len(frequency)), frequency.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. モデルのの実装\n",
    "# ResNetを利用できるようにしておく\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, stride: int = 1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "\n",
    "        out += self.shortcut(residual)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class BottleneckBlock(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, stride: int = 1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, stride=1)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels * self.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * self.expansion, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels * self.expansion)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "\n",
    "        out += self.shortcut(residual)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers):\n",
    "        super().__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, layers[0], 64)\n",
    "        self.layer2 = self._make_layer(block, layers[1], 128, stride=2)\n",
    "        self.layer3 = self._make_layer(block, layers[2], 256, stride=2)\n",
    "        self.layer4 = self._make_layer(block, layers[3], 512, stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, 512)\n",
    "\n",
    "    def _make_layer(self, block, blocks, out_channels, stride=1):\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride))\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "\n",
    "def ResNet50():\n",
    "    return ResNet(BottleneckBlock, [3, 4, 6, 3])\n",
    "\n",
    "class VQAModel(nn.Module):\n",
    "    def __init__(self, n_answer: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        self.bert_model = BertModel.from_pretrained(\n",
    "            \"bert-base-uncased\", torch_dtype=torch.float32, attn_implementation=\"sdpa\"\n",
    "        )\n",
    "        # self.bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        for name, param in self.bert_model.named_parameters():\n",
    "            if name.startswith('encoder.layer.11'):\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.resnet = ResNet18()\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(1282, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, n_answer)\n",
    "        )\n",
    "\n",
    "    def forward(self, image, question, color, closed):\n",
    "        N = image.shape[0]\n",
    "        image_feature = self.resnet(image)\n",
    "        assert image_feature.shape == (N, 512)\n",
    "\n",
    "        # with torch.no_grad():\n",
    "        tokens = self.bert_tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        tokens = {k: v.to(image.device) for k, v in tokens.items()}\n",
    "        question_feature = self.bert_model(**tokens).last_hidden_state[:, 0, :]\n",
    "        assert question_feature.shape == (N, 768)\n",
    "\n",
    "        color_features = color.clone().detach().view(N, 1).to(image.device)\n",
    "        closed_features = closed.clone().detach().view(N, 1).to(image.device)\n",
    "\n",
    "        x = torch.cat([image_feature, question_feature, color_features, closed_features], dim=1)\n",
    "        assert x.shape == (N, 1282)\n",
    "        x = self.fc(x)\n",
    "        x = torch.nn.functional.softmax(x, dim=1)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VQADataset(df_path=\"./data/train.json\", image_dir=\"./data/train\", transform=transform)\n",
    "test_dataset = VQADataset(df_path=\"./data/valid.json\", image_dir=\"./data/valid\", transform=transform, answer=False)\n",
    "test_dataset.update_dict(train_dataset)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=os.cpu_count(), pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=os.cpu_count(), pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. 学習の実装\n",
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    simple_acc = 0\n",
    "\n",
    "    start = time.time()\n",
    "    for image, question, answers, mode_answer in dataloader:\n",
    "        image, answers, mode_answer = \\\n",
    "            image.to(device, non_blocking=True), answers.to(device, non_blocking=True), mode_answer.to(device, non_blocking=True)\n",
    "\n",
    "        pred = model(image, question)\n",
    "        loss = criterion(pred, mode_answer.squeeze())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_acc += VQA_criterion(pred.argmax(1), answers)  # VQA accuracy\n",
    "        simple_acc += (pred.argmax(1) == mode_answer).float().mean().item()  # simple accuracy\n",
    "\n",
    "    return total_loss / len(dataloader), total_acc / len(dataloader), simple_acc / len(dataloader), time.time() - start\n",
    "\n",
    "\n",
    "def eval(model, dataloader, optimizer, criterion, device):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    simple_acc = 0\n",
    "\n",
    "    start = time.time()\n",
    "    for image, question, answers, mode_answer in dataloader:\n",
    "        image, answers, mode_answer = \\\n",
    "            image.to(device, non_blocking=True), answers.to(device, non_blocking=True), mode_answer.to(device, non_blocking=True)\n",
    "\n",
    "        pred = model(image, question)\n",
    "        loss = criterion(pred, mode_answer.squeeze())\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_acc += VQA_criterion(pred.argmax(1), answers)  # VQA accuracy\n",
    "        simple_acc += (pred.argmax(1) == mode_answer).mean().item()  # simple accuracy\n",
    "\n",
    "    return total_loss / len(dataloader), total_acc / len(dataloader), simple_acc / len(dataloader), time.time() - start\n",
    "\n",
    "\n",
    "def custom_train_collate_fn(batch):\n",
    "    images = [item[0] for item in batch]\n",
    "    input_ids = [item[1] for item in batch]\n",
    "    attention_masks = [item[2] for item in batch]\n",
    "    answers = [item[3] for item in batch]\n",
    "    mode_answers = [item[4] for item in batch]\n",
    "\n",
    "    images = torch.stack(images)\n",
    "\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "    attention_masks = pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "\n",
    "    answers = torch.stack(answers)\n",
    "    mode_answers = torch.stack([torch.tensor(ma) for ma in mode_answers])\n",
    "    # mode_answers = torch.stack([torch.tensor(item[4], dtype=torch.long) for item in batch])\n",
    "\n",
    "    return images, input_ids, attention_masks, answers, mode_answers\n",
    "\n",
    "\n",
    "def custom_test_collate_fn(batch):\n",
    "    images = [item[0] for item in batch]\n",
    "    input_ids = [item[1] for item in batch]\n",
    "    attention_masks = [item[2] for item in batch]\n",
    "\n",
    "    images = torch.stack(images)\n",
    "\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "    attention_masks = pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "\n",
    "    return images, input_ids, attention_masks\n",
    "\n",
    "\n",
    "class gcn():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, x):\n",
    "        mean = torch.mean(x)\n",
    "        std = torch.std(x)\n",
    "        return (x - mean)/(std + 10**(-6))  # 0除算を防ぐ\n",
    "    \n",
    "\n",
    "def main():\n",
    "    global print_cnt\n",
    "    # deviceの設定\n",
    "    set_seed(42)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # dataloader / model\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomRotation(degrees=(-180, 180)),  # random rotation\n",
    "        transforms.RandomCrop(32, padding=(4, 4, 4, 4), padding_mode='constant'),  # random cropping\n",
    "        transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5),\n",
    "        transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
    "        transforms.ToTensor(),\n",
    "        gcn()\n",
    "    ])\n",
    "\n",
    "    train_dataset = VQADataset(df_path=\"./data/train.json\", image_dir=\"./data/train\", transform=transform)\n",
    "    test_dataset = VQADataset(df_path=\"./data/valid.json\", image_dir=\"./data/valid\", transform=transform, answer=False)\n",
    "    test_dataset.update_dict(train_dataset)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=os.cpu_count(), pin_memory=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=os.cpu_count(), pin_memory=True)\n",
    "\n",
    "    model = VQAModel(n_answer=len(train_dataset.answer2idx)).to(device)\n",
    "\n",
    "    unanswerable_idx = train_dataset.answer2idx.get('unanswerable', None)\n",
    "    weights = torch.ones(len(train_dataset.answer2idx))\n",
    "    if unanswerable_idx is not None:\n",
    "        weights[unanswerable_idx] = 0.001\n",
    "    # optimizer / criterion\n",
    "    num_epoch = 5\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights.to(device))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "    # train model\n",
    "    for epoch in range(num_epoch):\n",
    "        train_loss, train_acc, train_simple_acc, train_time = train(model, train_loader, optimizer, criterion, device)\n",
    "        print(f\"【{epoch + 1}/{num_epoch}】\\n\"\n",
    "              f\"train time: {train_time:.2f} [s]\\n\"\n",
    "              f\"train loss: {train_loss:.4f}\\n\"\n",
    "              f\"train acc: {train_acc:.4f}\\n\"\n",
    "              f\"train simple acc: {train_simple_acc:.4f}\")\n",
    "        print_cnt = 0\n",
    "\n",
    "    # 提出用ファイルの作成\n",
    "    model.eval()\n",
    "    submission = []\n",
    "    for image, question in test_loader:\n",
    "        image = image.to(device)\n",
    "        pred = model(image, question)\n",
    "        pred = pred.argmax(1).cpu().item()\n",
    "        submission.append(pred)\n",
    "\n",
    "    submission = [train_dataset.idx2answer[id] for id in submission]\n",
    "    submission = np.array(submission)\n",
    "    torch.save(model.state_dict(), \"model.pth\")\n",
    "    np.save(\"submission.npy\", submission)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.word_embeddings.weight torch.Size([30522, 768])\n",
      "embeddings.position_embeddings.weight torch.Size([512, 768])\n",
      "embeddings.token_type_embeddings.weight torch.Size([2, 768])\n",
      "embeddings.LayerNorm.weight torch.Size([768])\n",
      "embeddings.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.0.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.0.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.0.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.0.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.0.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.0.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.0.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.0.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.0.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.0.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.0.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.0.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.0.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.0.output.dense.bias torch.Size([768])\n",
      "encoder.layer.0.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.0.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.1.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.1.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.1.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.1.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.1.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.1.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.1.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.1.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.1.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.1.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.1.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.1.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.1.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.1.output.dense.bias torch.Size([768])\n",
      "encoder.layer.1.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.1.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.2.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.2.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.2.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.2.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.2.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.2.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.2.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.2.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.2.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.2.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.2.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.2.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.2.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.2.output.dense.bias torch.Size([768])\n",
      "encoder.layer.2.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.2.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.3.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.3.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.3.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.3.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.3.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.3.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.3.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.3.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.3.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.3.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.3.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.3.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.3.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.3.output.dense.bias torch.Size([768])\n",
      "encoder.layer.3.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.3.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.4.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.4.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.4.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.4.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.4.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.4.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.4.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.4.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.4.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.4.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.4.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.4.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.4.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.4.output.dense.bias torch.Size([768])\n",
      "encoder.layer.4.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.4.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.5.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.5.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.5.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.5.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.5.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.5.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.5.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.5.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.5.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.5.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.5.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.5.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.5.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.5.output.dense.bias torch.Size([768])\n",
      "encoder.layer.5.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.5.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.6.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.6.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.6.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.6.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.6.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.6.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.6.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.6.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.6.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.6.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.6.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.6.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.6.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.6.output.dense.bias torch.Size([768])\n",
      "encoder.layer.6.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.6.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.7.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.7.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.7.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.7.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.7.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.7.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.7.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.7.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.7.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.7.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.7.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.7.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.7.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.7.output.dense.bias torch.Size([768])\n",
      "encoder.layer.7.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.7.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.8.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.8.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.8.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.8.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.8.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.8.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.8.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.8.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.8.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.8.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.8.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.8.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.8.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.8.output.dense.bias torch.Size([768])\n",
      "encoder.layer.8.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.8.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.9.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.9.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.9.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.9.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.9.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.9.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.9.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.9.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.9.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.9.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.9.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.9.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.9.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.9.output.dense.bias torch.Size([768])\n",
      "encoder.layer.9.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.9.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.10.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.10.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.10.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.10.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.10.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.10.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.10.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.10.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.10.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.10.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.10.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.10.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.10.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.10.output.dense.bias torch.Size([768])\n",
      "encoder.layer.10.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.10.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.11.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.11.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.11.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.11.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.11.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.11.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.11.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.11.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.11.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.11.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.11.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.11.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.11.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.11.output.dense.bias torch.Size([768])\n",
      "encoder.layer.11.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.11.output.LayerNorm.bias torch.Size([768])\n",
      "pooler.dense.weight torch.Size([768, 768])\n",
      "pooler.dense.bias torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "aa = BertModel.from_pretrained(\n",
    "            \"bert-base-uncased\", torch_dtype=torch.float32, attn_implementation=\"sdpa\"\n",
    "        )\n",
    "for name, param in aa.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
